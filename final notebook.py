# -*- coding: utf-8 -*-
"""notebook_6 (otw final).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dkLWbScxEPU2-5y0zOjGf2lkCoWU4_ck

# Books Recommendation System

Problem Statements:
- Bagaimana membangun sistem rekomendasi yang dapat mempersonalisasi daftar bacaan pengguna berdasarkan preferensi eksplisit (rating) dan implisit (pola interaksi)?
- Bagaimana memanfaatkan metadata buku (judul, penulis, genre) untuk menghasilkan rekomendasi yang relevan secara konten?
- Bagaimana mengukur dan membandingkan performa antara model Collaborative Filtering (CF) dan Content-Based Filtering (CBF) dalam sistem rekomendasi buku?

## Import Library
"""

import pandas as pd
import gdown
import matplotlib.pyplot as plt
import seaborn as sns
import re

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping

"""## Data Collection"""

# Download Books.csv
gdown.download('https://drive.google.com/uc?id=1IrvVMbxdK_GIUGCVWMdKE_Ty23PqPBZP',
               'Books.csv', quiet=False)

# Download Ratings.csv
gdown.download('https://drive.google.com/uc?id=1gYAqXCFk2TA-nAg2bimXfYGkYn0v6Gy-',
               'Ratings.csv', quiet=False)

# Baca file CSV
books = pd.read_csv('Books.csv', encoding='latin-1', low_memory=False)
ratings = pd.read_csv('Ratings.csv', encoding='latin-1')

books.head()

ratings.head()

"""## Exploratory Data Analysis (EDA)"""

# Ukurat dataset
print(f"Books dataset shape: {books.shape}")
print(f"Ratings dataset shape: {ratings.shape}")

"""### Univariate EDA (books)"""

books.head()

books.info()

"""keterangan: perlu diperiksa missing values semua kolom, konversi tahun publikasi ke int/datetime dan data non numerik, dan dropping variabel non relevan"""

books.isnull().sum()

"""keterangan: akan dilakukan imputasi nantinya karena variabel dianggap penting untuk CBF. nantinya NaN akan diubah menjadi string 'Unknown Author/Publisher'"""

print(f"Jumlah duplikat baris data: {books.duplicated().sum()}")

print(f"Jumlah duplikat ISBN: {books['ISBN'].duplicated().sum()}") # Primary key unik (no duplicate)

# Cetak semua tahun terbit yang unik
unique_years = books['Year-Of-Publication'].unique()
print(unique_years)

"""keterangan: setelah diperiksa ternyata ada variabel non numerik (bukan tahun) di dalam variabel 'Year-of-Publication'. itu adalah data kotor yang mungkin saja terjadi karena ada salah ketik."""

# Top 10 penulis dengan buku terbanyak
top_authors = books['Book-Author'].value_counts().head(10)

plt.figure(figsize=(10,6))
sns.barplot(y=top_authors.index, x=top_authors.values, palette='crest')
plt.title('Top 10 Penulis dengan Jumlah Buku Terbanyak')
plt.xlabel('Jumlah Buku')
plt.ylabel('Penulis')
plt.show()

# Top 10 penerbit dengan buku terbanyak
top_publishers = books['Publisher'].value_counts().head(10)

plt.figure(figsize=(10,6))
sns.barplot(y=top_publishers.index, x=top_publishers.values, palette='viridis')
plt.title('Top 10 Penerbit dengan Buku Terbanyak')
plt.xlabel('Jumlah Buku')
plt.ylabel('Penerbit')
plt.show()

# Top 10 buku paling umum
print("Jumlah judul unik:", books['Book-Title'].nunique())
print("Judul buku paling umum:")
print(books['Book-Title'].value_counts().head(10))

# Distribusi panjang ISBN untuk validasi format
books['ISBN_Length'] = books['ISBN'].astype(str).apply(len)
plt.figure(figsize=(8,4))
sns.countplot(x='ISBN_Length', data=books, palette='Set2')
plt.title('Distribusi Panjang ISBN')
plt.xlabel('Jumlah Karakter')
plt.ylabel('Jumlah Buku')
plt.show()

"""### Univariate EDA (ratings)

"""

ratings.head()

ratings.info()

print(ratings.isnull().sum())

print(f"Jumlah duplikat User-ID: {ratings['User-ID'].duplicated().sum()}")

"""keterangan: User-ID memang muncul berulang kali di dataset rating, karena satu user bisa memberi rating ke banyak buku. Sehingga adanya duplikat itu normal"""

print(f"Jumlah duplikat baris data: {ratings.duplicated().sum()}")

# Frekuensi setiap rating
rating_counts = ratings['Book-Rating'].value_counts().sort_index()

plt.figure(figsize=(10,6))
sns.barplot(x=rating_counts.index, y=rating_counts.values, palette='viridis')
plt.title('Distribusi Nilai Book-Rating')
plt.xlabel('Book Rating')
plt.ylabel('Jumlah')
plt.xticks(rotation=0)
plt.show()

"""keterangan: Book Rating '0' berarti User-ID mungkin tidak memberikan rating eksplisit. rating tersebut bisa dianggap user pernah berinteraksi dengan buku (hanya melihat) dan bukan penilaian sebenarnya. sehingga nantinya nilai tersebut akan dihilangkan saja dan hanya mengandalkan rating eksplisit (1-10) saja untuk CF dan CBF."""

user_rating_counts = ratings['User-ID'].value_counts()

plt.figure(figsize=(10,6))
sns.histplot(user_rating_counts, bins=50, kde=True, color='coral')
plt.title('Distribusi Jumlah Rating per User')
plt.xlabel('Jumlah Rating per User')
plt.ylabel('Jumlah User')
plt.xlim(0, 200)
plt.show()

"""keterangan: banyak user yang hanya memberi sedikit rating sehingga nantinya akan dihapus user yang pasif agar pada saat CF, data tidak menjadi sparse. akibatnya jika pasif (hanya 1-2 rating) maka matriks CF nantinya akan jarang terisi sehingga berpengaruh ke output rekomendasi"""

book_rating_counts = ratings['ISBN'].value_counts()

plt.figure(figsize=(10,6))
sns.histplot(book_rating_counts, bins=50, kde=True, color='teal')
plt.title('Distribusi Jumlah Rating per Buku')
plt.xlabel('Jumlah Rating per Buku')
plt.ylabel('Jumlah Buku')
plt.xlim(0, 100)
plt.show()

"""keterangan: banyak buku dengan hanya sedikit rating sehingga untuk CF nanti performa bisa drop karena kurang cukup untuk mengenali kemiripan item/user"""

plt.figure(figsize=(10,6))
sns.boxplot(x='Book-Rating', data=ratings, palette='coolwarm', showfliers=True)
plt.title('Boxplot Lengkap Distribusi Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Frekuensi')
plt.xticks(rotation=0)
plt.show()

"""## Data Preparation"""

# Drop kolom yang tidak relevan
books = books.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L', 'ISBN_Length'])

# missing value diubah menjadi 'Unknown'
books['Book-Author'].fillna('Unknown Author', inplace=True)
books['Publisher'].fillna('Unknown Publisher', inplace=True)

# Normalisasi teks: hapus spasi ekstra
def clean_text(text):
    if pd.isnull(text):
        return ""

    text = text.strip()                      # hapus spasi di awal/akhir
    text = re.sub(r'\s+', ' ', text)        # hapus spasi berlebih di tengah
    return text

for col in ['Book-Title', 'Book-Author', 'Publisher']:
    books[col] = books[col].apply(clean_text)

books['Year-Of-Publication'].unique()

"""keterangan: 'Year-of-Publication' sengaja tidak diimputasi maupun di drop karena variabel tersebut bukan fokus untuk CF maupun CBF. Sehingga adanya kesalahan input dibiarkan sebagaimana data aslinya."""

# Filter user aktif
user_counts = ratings['User-ID'].value_counts()
ratings = ratings[ratings['User-ID'].isin(user_counts[user_counts >= 3].index)]

# Filter buku populer
book_counts = ratings['ISBN'].value_counts()
ratings = ratings[ratings['ISBN'].isin(book_counts[book_counts >= 3].index)]

# Menghilangkan rating 0 (tidak eksplisit)
ratings = ratings[ratings['Book-Rating'] > 0]

# Membuat cf_data dan cbf_data khusus modelling
cf_data = ratings.merge(books[['ISBN',
                               'Book-Title']], on='ISBN', how='left')

cbf_data = ratings.merge(books[['ISBN',
                                'Book-Title',
                                'Book-Author']], on='ISBN', how='left')

cbf_data.head()

cbf_data.isnull().sum()

"""keterangan: Nilai NaN di kolom Book-Title dan Book-Author muncul karena tidak semua ISBN di ratings ada di books sehingga informasi buku tidak lengkap di hasil merge. Untuk kasus content based, adanya nilai NaN akan memengaruhi ketepatan hasil rekomendasi sehingga perlu adanya drop nilai"""

cbf_data = cbf_data.dropna(subset=['Book-Title', 'Book-Author'])

cbf_data.info()

cf_data.head()

cf_data.isnull().sum()

"""keterangan: sama seperti kasus pada cbf_data, tetapi pada kasus ini, 'Book-Title' yang kosong dibiarkan saja karena tidak digunakan dalam CF

## Modelling & Evaluation

### Content Based Filtering (CBF)
"""

cbf_data.head()

cbf_data.info()

# Menampilkan jumlah judul buku yang duplikat
duplicate_titles = cbf_data['Book-Title'].duplicated().sum()
print(f"Jumlah duplikat Book-Title: {duplicate_titles}")

"""keterangan: hal tersebut bisa terjadi apabila buku dibaca > 1 user, ISBN beda tadi judul sama, atau adanya variasi penulis dan publisher untuk judul yang sama

keterangan: untuk mencegah google colab crash, maka hanya digunakan beberapa baris data saja agar tidak menghasilkan matrix yang sangat besar. namun, model CBF tetap dapat berjalan dengan dampak menurunnya nilai similarity karena ada buku buku yang mungkin tidak termasuk dalam data sample.
"""

# Diambil 5000 data yang unik saja
cbf_data_sample = cbf_data.drop_duplicates(subset='Book-Title').head(5000).reset_index(drop=True)

# Mmebuat fitur gabungan
cbf_data_sample['content'] = (
    cbf_data_sample['Book-Title'].fillna('') + ' ' +
    cbf_data_sample['Book-Author'].fillna('')
).str.lower()

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(cbf_data_sample['content'])

# Hitung cosine similarity
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Membuat mapping
indices = pd.Series(cbf_data_sample.index, index=cbf_data_sample['Book-Title'].str.lower()).drop_duplicates()

def recommend_books_cbf(title, top_n=5):
    # Normalisasi judul agar tidak case-sensitive
    title = title.lower()

    if title not in indices:
        return f"Buku '{title}' tidak ditemukan."

    # Ambil indeks dari buku yang dicari
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort dan ambil top n kecuali dirinya sendiri
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]

    # Ambil indeks buku-buku yang paling mirip
    book_indices = [i[0] for i in sim_scores]
    similarities = [round(i[1], 3) for i in sim_scores]

    recommended = cbf_data_sample.iloc[book_indices][['Book-Title', 'Book-Author']].reset_index(drop=True)
    recommended['Similarity'] = similarities

    return recommended

# Contoh penggunaan
recommend_books_cbf('See Jane Run')

"""keterangan: nilai 'Similarity' akan semakin tinggi apabila variasi atau jumlah sample cbf_data semakin besar. namun, karena keterbatasan google cloud, sehingga hanya 5000 unique value saja yang dapat diolah."""

# Metrik evaluasi (Precision) untuk model CBF
def evaluate_cbf_precision(anchor_title, liked_books, top_n=5):
    recommendations = recommend_books_cbf(anchor_title, top_n=top_n)

    # Ambil daftar judul hasil rekomendasi
    recommended_titles = recommendations['Book-Title'].str.lower().tolist()
    liked_books_normalized = [book.lower() for book in liked_books] # list buku yang mungkin disukai user

    # Hitung jumlah rekomendasi yang juga ada di liked_books
    relevant = sum(1 for book in recommended_titles if book in liked_books_normalized)

    # Hitung precision
    precision = relevant / top_n

    print("Anchor Book:", anchor_title)
    print("Recommended Books:", recommended_titles)
    print(f"Precision@{top_n}: {precision:.2f}")

    return precision

# Penggunaan fungsi evaluasi
anchor_title = 'See Jane Run'

liked_books = [
    'The First Time',
    'Run Jane Run',
    'Grand Avenue',
    'Lauf, Jane, lauf. Roman.',
    'Olivia Joules and the Overactive Imagination'
]

evaluate_cbf_precision(anchor_title, liked_books, top_n=5)

"""### Collaborative Filtering (CF)"""

cf_data.head()

cf_data_original = cf_data.copy() # Menyalin dataset asli agar tetap utuh

# Encoding User-ID dan ISBN
user_ids = cf_data['User-ID'].unique().tolist() # mengambil daftar unik variabel
isbn_ids = cf_data['ISBN'].unique().tolist()

# mapping dari variabel asli ke ID numerik urut
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_ids)}
user_encoded_to_user = {i: x for x, i in user_to_user_encoded.items()}
isbn_encoded_to_isbn = {i: x for x, i in isbn_to_isbn_encoded.items()}

# mengganti index dengan hasil encoding (ID numerik)
cf_data['User-ID'] = cf_data['User-ID'].map(user_to_user_encoded)
cf_data['ISBN'] = cf_data['ISBN'].map(isbn_to_isbn_encoded)

num_users = len(user_to_user_encoded)
num_books = len(isbn_to_isbn_encoded)

min_rating = cf_data['Book-Rating'].min()
max_rating = cf_data['Book-Rating'].max()

# Mengacak urutan data untuk memastikan distribusi acak saat training
cf_data = cf_data.sample(frac=1, random_state=42)

# memisahkan fitur x dan y
x = cf_data[['User-ID', 'ISBN']].values
y = cf_data['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values # normalisasi untuk kemudahan saat training

# Menentukan indeks batas untuk split
train_indices = int(0.8 * cf_data.shape[0])

# Membagi data menjadi data latih (train) dan data validasi (val)
x_train, x_val = x[:train_indices], x[train_indices:]
y_train, y_val = y[:train_indices], y[train_indices:]

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_books, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)

        # Layer embedding untuk user
        self.user_embedding = layers.Embedding(
            input_dim=num_users,                    # Jumlah user unik
            output_dim=embedding_size,              # Ukuran vektor embedding
            embeddings_initializer='he_normal',     # Inisialisasi bobot
            embeddings_regularizer=keras.regularizers.l2(1e-6)  # Regularisasi L2 untuk mencegah overfitting
        )

        # Bias khusus untuk tiap user
        self.user_bias = layers.Embedding(num_users, 1)

        # Layer embedding untuk book
        self.book_embedding = layers.Embedding(
            input_dim=num_books,                    # Jumlah buku unik
            output_dim=embedding_size,              # Ukuran vektor embedding
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )

        # Bias khusus untuk tiap buku
        self.book_bias = layers.Embedding(num_books, 1)

    def call(self, inputs):
        # Mengambil vektor embedding dan bias berdasarkan input user
        user_vector = self.user_embedding(inputs[:, 0])  # inputs[:, 0] adalah user-ID
        user_bias = self.user_bias(inputs[:, 0])

        # Mengambil vektor embedding dan bias berdasarkan input buku
        book_vector = self.book_embedding(inputs[:, 1])  # inputs[:, 1] adalah book-ID
        book_bias = self.book_bias(inputs[:, 1])

        # Menghitung dot product antara vektor user dan book (interaksi)
        dot_user_book = tf.reduce_sum(user_vector * book_vector, axis=1, keepdims=True)

        # Menambahkan bias user dan bias book ke hasil dot product
        x = dot_user_book + user_bias + book_bias

        # Menggunakan sigmoid agar output berada pada rentang [0, 1]
        return tf.nn.sigmoid(tf.squeeze(x, axis=1))

model = RecommenderNet(num_users, num_books, embedding_size=50)

model.compile(
    loss=tf.keras.losses.MeanSquaredError(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Callback untuk menghentikan training saat model mulai overfitting
early_stopping = EarlyStopping(
    monitor='val_root_mean_squared_error',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=64,
    epochs=100,
    validation_data=(x_val, y_val),
    callbacks=[early_stopping]
)

# Visualisasi metrik RMSE selama proses training
plt.figure(figsize=(8, 5))
plt.plot(history.history['root_mean_squared_error'], label='Train RMSE', marker='o')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE', marker='o')
plt.title('Root Mean Squared Error (RMSE) per Epoch')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Ambil 1 user acak dari data encoded
user_id = cf_data['User-ID'].sample(1).iloc[0]
books_read_by_user = cf_data[cf_data['User-ID'] == user_id] # mencari buku yang sudah dibaca user

# Cari buku yang belum dibaca user (encoded)
book_not_read = [x for x in range(num_books) if x not in books_read_by_user['ISBN'].values]
book_not_read = [[x] for x in book_not_read]
user_book_array = np.hstack(([[user_id]] * len(book_not_read), book_not_read))

# Prediksi rating buku yang belum dibaca
ratings = model.predict(user_book_array, verbose=0).flatten()
top_ratings_indices = ratings.argsort()[-5:][::-1]

# Mendapatkan ISBN asli dari indeks encoded
recommended_book_ids = [
    isbn_encoded_to_isbn[book_not_read[i][0]] for i in top_ratings_indices
]

print(f"Rekomendasi untuk User-ID: {user_encoded_to_user[user_id]}")
print("=" * 30)

# Buku yang disukai user (Top 5 tertinggi dari rating user sebelumnya)
print("Buku yang disukai user:")
top_books_user = books_read_by_user.sort_values(by='Book-Rating', ascending=False).head(5)

for i in top_books_user['ISBN']:
    isbn_asli = isbn_encoded_to_isbn[i]  # konversi lagi ke ISBN asli
    matching_books = cf_data_original[cf_data_original['ISBN'] == isbn_asli]['Book-Title'].values
    if len(matching_books) > 0:
        print("-", matching_books[0])
    else:
        print("-", f"Judul tidak ditemukan untuk ISBN: {isbn_asli}")

# Top 5 Rekomendasi dari model
print("\nTop 5 Rekomendasi Buku:")
for isbn in recommended_book_ids:
    matching_books = cf_data_original[cf_data_original['ISBN'] == isbn]['Book-Title'].values
    if len(matching_books) > 0:
        print("-", matching_books[0])
    else:
        print("-", f"Judul tidak ditemukan untuk ISBN: {isbn}")